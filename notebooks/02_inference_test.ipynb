{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3943c",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb990652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = \"../models/sentiment_model.pkl\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model file not found at {model_path}\")\n",
    "    print(\"Please train the model first using the training notebook or script.\")\n",
    "else:\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "    print(f\"\\nModel pipeline steps:\")\n",
    "    for name, step in model.named_steps.items():\n",
    "        print(f\"  - {name}: {type(step).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df4022",
   "metadata": {},
   "source": [
    "## 2. Single Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single text\n",
    "text = \"This product is absolutely amazing! I love it!\"\n",
    "\n",
    "prediction = model.predict([text])[0]\n",
    "probabilities = model.predict_proba([text])[0]\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nPrediction: {prediction.upper()}\")\n",
    "print(f\"\\nProbabilities:\")\n",
    "print(f\"  Negative: {probabilities[0]:.4f}\")\n",
    "print(f\"  Positive: {probabilities[1]:.4f}\")\n",
    "print(f\"\\nConfidence: {max(probabilities):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e89240",
   "metadata": {},
   "source": [
    "## 3. Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f369cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple texts\n",
    "test_texts = [\n",
    "    \"Excellent product! Highly recommend!\",\n",
    "    \"Terrible experience. Very disappointed.\",\n",
    "    \"Good quality for the price.\",\n",
    "    \"Don't waste your money on this.\",\n",
    "    \"Best purchase I've made this year!\",\n",
    "    \"Poor quality. Broke after one use.\",\n",
    "    \"Love it! Works perfectly.\",\n",
    "    \"Not worth it. Save your money.\",\n",
    "    \"Outstanding performance and value!\",\n",
    "    \"Completely useless product.\"\n",
    "]\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_texts)\n",
    "probabilities = model.predict_proba(test_texts)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': test_texts,\n",
    "    'Prediction': predictions,\n",
    "    'Confidence': [max(p) for p in probabilities],\n",
    "    'Negative_Prob': [p[0] for p in probabilities],\n",
    "    'Positive_Prob': [p[1] for p in probabilities]\n",
    "})\n",
    "\n",
    "# Display results\n",
    "print(\"Batch Prediction Results:\")\n",
    "print(\"=\" * 100)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nPrediction Summary:\")\n",
    "print(f\"Total texts: {len(test_texts)}\")\n",
    "print(f\"Positive predictions: {sum(predictions == 'positive')}\")\n",
    "print(f\"Negative predictions: {sum(predictions == 'negative')}\")\n",
    "print(f\"\\nAverage confidence: {results_df['Confidence'].mean():.2%}\")\n",
    "print(f\"Min confidence: {results_df['Confidence'].min():.2%}\")\n",
    "print(f\"Max confidence: {results_df['Confidence'].max():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ccdcd",
   "metadata": {},
   "source": [
    "## 4. Interactive Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def predict_sentiment(text: str) -> None:\n",
    "    \"\"\"Predict sentiment for a given text.\"\"\"\n",
    "    if not text.strip():\n",
    "        print(\"Please enter some text.\")\n",
    "        return\n",
    "    \n",
    "    prediction = model.predict([text])[0]\n",
    "    probabilities = model.predict_proba([text])[0]\n",
    "    confidence = max(probabilities) * 100\n",
    "    \n",
    "    # Emoji based on sentiment\n",
    "    emoji = \"ðŸ˜Š\" if prediction == \"positive\" else \"ðŸ˜ž\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"\\n{emoji} Sentiment: {prediction.upper()}\")\n",
    "    print(f\"Confidence: {confidence:.1f}%\")\n",
    "    print(f\"\\nProbabilities:\")\n",
    "    print(f\"  Negative: {probabilities[0]:.1%}\")\n",
    "    print(f\"  Positive: {probabilities[1]:.1%}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Example usage\n",
    "predict_sentiment(\"This is wonderful! I'm so happy with it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own text\n",
    "# Uncomment and modify the line below to test your own text\n",
    "# predict_sentiment(\"Your custom text here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104562fd",
   "metadata": {},
   "source": [
    "## 5. Test with Seldon-Compatible Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1dbe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Seldon input format\n",
    "def predict_seldon_format(texts: list) -> dict:\n",
    "    \"\"\"Make predictions in Seldon-compatible format.\"\"\"\n",
    "    predictions = model.predict(texts)\n",
    "    probabilities = model.predict_proba(texts)\n",
    "    \n",
    "    # Format similar to Seldon response\n",
    "    return {\n",
    "        \"data\": {\n",
    "            \"names\": [\"negative\", \"positive\"],\n",
    "            \"ndarray\": predictions.tolist()\n",
    "        },\n",
    "        \"meta\": {\n",
    "            \"probabilities\": probabilities.tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_input = [\"Great product!\", \"Terrible quality.\"]\n",
    "result = predict_seldon_format(test_input)\n",
    "\n",
    "print(\"Seldon-format prediction:\")\n",
    "import json\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45bcf0",
   "metadata": {},
   "source": [
    "## 6. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0450b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"\\nPipeline steps:\")\n",
    "for i, (name, step) in enumerate(model.named_steps.items(), 1):\n",
    "    print(f\"  {i}. {name}: {type(step).__name__}\")\n",
    "    if hasattr(step, 'get_params'):\n",
    "        params = step.get_params()\n",
    "        if name == 'tfidf':\n",
    "            print(f\"     - Max features: {params.get('max_features')}\")\n",
    "            print(f\"     - N-gram range: {params.get('ngram_range')}\")\n",
    "        elif name == 'classifier':\n",
    "            print(f\"     - Algorithm: {type(step).__name__}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
